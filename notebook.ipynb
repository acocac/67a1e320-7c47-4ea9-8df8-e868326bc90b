{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sea ice forecasting using the IceNet library\n",
    "\n",
    "## Context\n",
    "### Purpose\n",
    "This notebook demonstrates the use of the [IceNet library](https://github.com/icenet-ai/icenet/) for sea-ice forecasting trained using climate reanalysis and observational data.\n",
    "\n",
    "### Description\n",
    "[IceNet](https://github.com/icenet-ai/icenet/) is a python library that provides the ability to download, process, train and predict from end to end. Users can interact with IceNet either via the python interface or via a set of command-line interfaces (CLI) which provide a high-level interface that covers the above abilities.\n",
    "\n",
    "This notebook demonstrates the use of the python library api for forecasting sea ice for a reduced dataset to demonstrate its capabilities.\n",
    "\n",
    "### Modelling approach\n",
    "IceNet is a probabilistic, deep learning sea ice forecasting system. It utilises ensemble modelling of U-Net networks to generate daily forecasts of sea ice condition, trained on climate reanalysis and sea ice observational data at 25km resolution. The original IceNet research model, published in [Nature Communications](https://www.nature.com/articles/s41467-021-25257-4) was trained on climate simulations and observational data to forecast the next 6 months of monthly-averaged sea ice concentration maps. This version advanced the range of accurate sea ice forecasts, outperforming a state-of-the-art dynamical model (ECMWF SEAS5) in seasonal forecasts of summer sea ice, particularly for extreme sea ice events.\n",
    "\n",
    "### Highlights\n",
    "*Provide 3-5 bullet points that convey the use case’s core procedures. Each bullet point must have a maximum of 85 characters, including spaces.*\n",
    "\n",
    " * [1. Setup](#Setup): Set up the environment and project structure.\n",
    " * [2. Download](#Download): Download sea ice concentration data for training using the built-in downloader for first quarter of the year 2020.\n",
    " * [3. Process](#Process): Process the downloaded data by renormalising variables as needed, and generating cached datasets to speed up training.\n",
    " * [4. Train](#Train): Train the neural network and generate checkpoint best result.\n",
    " * [5. Predict](#Predict): Predict for defined dates.\n",
    " * [6. Visualisation](#Visualisation): Visualise the prediction output.\n",
    "\n",
    "### Contributions\n",
    "\n",
    "#### Notebook\n",
    "* James Byrne (Notebook Author), British Antarctic Survey, [@JimCircadian](https://github.com/JimCircadian)\n",
    "* Bryn Noel Ubald (Notebook Author), British Antarctic Survey, [@bnubald](https://github.com/bnubald)\n",
    "\n",
    "#### Modelling codebase\n",
    "* James Byrne (Code author)\n",
    "* Tom Andersson (Science author)\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/icenet-ai/icenet-notebooks/issues) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., Pérez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "> [!NOTE]  \n",
    "> \n",
    "> IceNet has developed significantly since the inclusion of the existing notebook (relates to issue #6) based on the work in the original paper.\n",
    ">\n",
    "> The original paper and notebook used a combination of climate simulations and observational data to forecast the next 6 months of monthly-averaged sea ice concentration. Since then, the original code has been refactored into a new library icenet as showcased in this notebook.\n",
    "> This library supports sea ice forecasting on a daily resolution rather than monthly-averaged. It has been developed significantly since the original paper, and there are multiple ways of interacting with the library to help enable development of sea ice forecasting and model development.\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "Load some of the common libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports modules from the IceNet library as preparation for the downloaders. Whose instantiation describes the interactions with the upstream APIs/data interfaces used to source various types of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set project structure\n",
    "*The cell below creates a separate folder to save the notebook outputs. This facilitates the reader to inspect inputs/outputs stored within a defined destination folder. Don't remove the lines below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "notebook_folder = './notebook'\n",
    "if not os.path.exists(notebook_folder):\n",
    "    os.makedirs(notebook_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Download\n",
    "\n",
    "In this section, we download all required data with our extended date range. All downloaders inherit a `download` method from the `Downloader` class in [`icenet.data.producers`](https://github.com/icenet-ai/icenet/blob/main/icenet/data/producers.py), which also contains two other data producing classes `Generator` (which Masks inherits from) and `Processor` (used in the next section), each providing abstract implementations that multiple classes derive from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data\n",
    "\n",
    "We start here with generating the masks for training/prediction. This includes regions where sea ice does not form, land regions, and the [polar hole](https://blogs.egu.eu/divisions/cr/2016/10/14/image-of-the-week-the-polar-hole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate and Ocean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climate and ocean data are obtained from the [Climate Data Store (CDS)](https://cds.climate.copernicus.eu/).\n",
    "\n",
    "The climate data used for training is from [ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) reanalysis which covers the global climate since 1940 to the present time.\n",
    "\n",
    "The Ocean data used is from [ORA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-oras5?tab=overview) which also uses a reanalysis approach and contains global ocean and sea-ice reanalysis data.\n",
    "\n",
    "Since these are both obtained from reanalysis, they are a combination of physical models and observational data. Due to the reanalysis approach, there is no temporal or spatial gap in the downloaded data. Both of these sets of data are obtained from the ECMWF's (European Centre for Medium-Range Weather Forecast) reanalysis systems.\n",
    "\n",
    "Please see the above links for more details on these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloader implementation of this data in IceNet utilises the CDS API which requires registration and configuration of a token before downloading. The registration is free, please see [here](https://cds.climate.copernicus.eu/api-how-to) for more instructions on how to set this up.\n",
    "\n",
    "Assuming you have configured the CDS API key correctly, you will be able to download using the following IceNet class. Since the key is personal and should not be shared, the call to download an example dataset is shown below, but not used in this demonstrator notebook.\n",
    "\n",
    "```python\n",
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],      # Name of variables to download\n",
    "    dates=[                                     # Dates to download the variable data for\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-04-31\", freq=\"D\")\n",
    "    ],\n",
    "    path=data_dir,                              # Location to download data to (default is `./data`)\n",
    "    delete_tempfiles=True,                      # Whether to delete temporary downloaded files\n",
    "    levels=[None, [250, 500], None, None],      # The levels at which to obtain the variables for (e.g. for zg, it is the pressure levels)\n",
    "    max_threads=4,                              # Maximum number of concurrent downloads\n",
    "    north=False,                                # Boolean: Whether require data across northern hemisphere\n",
    "    south=True,                                 # Boolean: Whether require data across southern hemisphere\n",
    "    use_toolbox=False)                          # Experimental, alternative download method\n",
    "era5.download()                                 # Start downloading\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ERA5Downloader` inherits from `ClimateDownloader`, from which several implementations derive their functionality. Two particularly useful methods shown below allow the downloaded data to be converted to the same grid and orientation as the OSISAF sea-ice concentration (SIC) data in the next cell.\n",
    "\n",
    "```python\n",
    "era5.regrid()                                   # Map data onto common EASE2 grid\n",
    "era5.rotate_wind_data()                         # Rotate wind data to correct orientation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea-ice concentration (SIC) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sea-ice concentration data use for training is obtained from [OSI SAF](https://osi-saf.eumetsat.int/products/sea-ice-products).\n",
    "\n",
    "The SIC is defined as the fraction of a grid cell that is covered in sea-ice.\n",
    "\n",
    "You will notice a familiar interface as with the `ERA5Downloader` class with the `SICDownloader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic = SICDownloader(\n",
    "    dates=[\n",
    "        pd.to_datetime(date).date()             # Dates to download the variable data for\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-03-31\", freq=\"D\")\n",
    "    ],\n",
    "    delete_tempfiles=True, # Whether to delete temporary downloaded files\n",
    "    north=False,           # Boolean: Whether to use mask for this region\n",
    "    south=True,            # Boolean: Whether to use mask for this region\n",
    "    parallel_opens=True,   # Boolean: Whether to use `dask.delayed` to open and preprocess multiple files in parallel\n",
    ")\n",
    "\n",
    "sic.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Process\n",
    "\n",
    "Similarly to the downloaders, each data producer (be it a `Downloader` or `Generator`) has a respective `Processor` that converts the `./notebook/data/` products into a normalised, preprocessed dataset under `./notebook/processed/`.\n",
    "\n",
    "Firstly, to make life a bit easier, we set up some variables. In this case we're creating a train/validate/test split out of the 2020 data in a fairly naive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-03-06\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-03-11\", \"2020-03-31\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-03-08\", \"2020-03-09\")],\n",
    ")\n",
    "processed_name = \"notebook_api_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These modules import the Processing modules for the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],                # Absolute normalised variables\n",
    "    [],                         # Variables defined as deviations from an aggregated norm\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True,\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrator does not use the ERA5 climate reanalysis data as mentioned above since an private API key should be set up per user to access the CDS API. However, if it was set up, the ERA5 data can also be preprocessed by:\n",
    "\n",
    "```python\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],             # Absolute normalised variables\n",
    "    [\"tas\", \"zg500\", \"zg250\"],  # Variables defined as deviations from an aggregated norm\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "Now, we can create a dataset configuration for training the network.This can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_data.json\"\n",
    "dataset_name = \"api_dataset\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=4,\n",
    "    generate_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loader config contains information about the data sources included and also the different dates to use for the training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl._config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.api_dataset.json` (recall we set the dataset name as `api_dataset` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distributed.nanny:Closing Nanny gracefully at 'tcp://127.0.0.1:35675'. Reason: worker-close\n",
      "INFO:distributed.nanny:Closing Nanny gracefully at 'tcp://127.0.0.1:42976'. Reason: worker-close\n",
      "INFO:distributed.scheduler:Remove client Client-worker-d2427ba6-e216-11ee-a878-ec2a7245f900\n",
      "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:36340; closing.\n",
      "INFO:distributed.scheduler:Remove client Client-worker-d2427ba6-e216-11ee-a878-ec2a7245f900\n",
      "INFO:distributed.scheduler:Remove client Client-worker-dd607a9a-e216-11ee-a874-ec2a7245f900\n",
      "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:36434; closing.\n",
      "INFO:distributed.scheduler:Remove client Client-worker-dd607a9a-e216-11ee-a874-ec2a7245f900\n",
      "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:36302; closing.\n",
      "INFO:distributed.scheduler:Close client connection: Client-worker-d2427ba6-e216-11ee-a878-ec2a7245f900\n",
      "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:39111', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710431106.6278787')\n",
      "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:36300; closing.\n",
      "INFO:distributed.scheduler:Close client connection: Client-worker-dd607a9a-e216-11ee-a874-ec2a7245f900\n",
      "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:35760', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710431106.6557734')\n",
      "INFO:distributed.scheduler:Lost all workers\n",
      "INFO:distributed.nanny:Worker process 43128 was killed by signal 15\n",
      "INFO:distributed.nanny:Worker process 43124 was killed by signal 15\n",
      "INFO:distributed.scheduler:Scheduler closing due to unknown reason...\n",
      "INFO:distributed.scheduler:Scheduler closing all comms\n"
     ]
    }
   ],
   "source": [
    "dl.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate samples from this dataset, we can use the `.generate_sample()` method, which returns the inputs `x`, `y` and sample weights `sw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, sw = dl.generate_sample(pd.Timestamp(\"2020-03-08\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(x): {type(x)}, x.shape: {x.shape}\")\n",
    "print(f\"type(y): {type(y)}, y.shape: {y.shape}\")\n",
    "print(f\"type(sw): {type(sw)}, sw.shape: {sw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Train\n",
    "\n",
    "For single runs we programmatically can call the same method used by the CLI. `train_model` defines the training process from start to finish. The [`model-ensembler`](https://github.com/JimCircadian/model-ensembler) works outside the API, controlling multiple CLI submissions. Customising an ensemble can be achieved through looking at the configuration in [the pipeline repository](https://github.com/antarctica/IceNet-Pipeline). That said, if workflow system integration (e.g. Airflow) is desired, integrating via this method is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSet\n",
    "\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\"\n",
    "dataset = IceNetDataSet(dataset_config, batch_size=4)\n",
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain the data loader that was used to create the dataset config via the `.get_data_loader()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `train_model` function to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.model.train import train_model\n",
    "\n",
    "run_name = \"api_test_run\"\n",
    "seed = 42\n",
    "\n",
    "trained_path, history = train_model(\n",
    "    run_name=\"api_test_run\",\n",
    "    dataset=dataset,\n",
    "    epochs=10,\n",
    "    n_filters_factor=0.3,\n",
    "    seed=seed,\n",
    "    strategy=strategy,\n",
    "    training_verbosity=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the training workflow is very standard for deep learning networks, with `train_model` wrapping up the training process with a lot of customisation of extraneous functionality.\n",
    "\n",
    "For a higher level of customisation programmatically, the training function can be split apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Predict\n",
    "\n",
    "In much the same manner as with `train_model`, the `predict_forecast` method acts as a convenient entry point workflow system integration, CLI entry as well as an overridable method upon which to base custom implementations. Using the method directly relies on loading from a prepared (but perhaps not cached) dataset.\n",
    "\n",
    "Some parameters are fed to `predict_forecast` that ideally shouldn't need to be specified (like `seed` and `n_filters_factor`) and might seem contextually odd. They're used to locate the appropriate saved network. *This will be cleaned up in a future version*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.model.predict import predict_forecast\n",
    "\n",
    "# Follows the naming convention used by the CLI version\n",
    "output_dir = os.path.join(\".\", \"results\", \"predict\",\n",
    "                          \"custom_run_forecast\",\n",
    "                          \"{}.{}\".format(run_name, \"42\"))\n",
    "\n",
    "predict_forecast(\n",
    "    dataset_config=dataset_config,\n",
    "    network_name=run_name,\n",
    "    n_filters_factor=0.3,\n",
    "    output_folder=output_dir,\n",
    "    seed=seed,\n",
    "    start_dates=[pd.to_datetime(el).date()\n",
    "                 for el in pd.date_range(\"2020-03-08\", \"2020-03-09\")],\n",
    "    test_set=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printf \"2020-04-01\\n2020-04-02\" | tee predict_dates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!icenet_output -m -o ./results/predict custom_run_forecast api_dataset predict_dates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "from IPython.display import HTML\n",
    "import xarray as xr, pandas as pd, datetime as dt\n",
    "\n",
    "# Load our output prediction file\n",
    "ds = xr.open_dataset(\"results/predict/custom_run_forecast.nc\")\n",
    "land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the forecast start date\n",
    "forecast_date = ds.time.values[0]\n",
    "print(forecast_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "fc['time'] = [pd.to_datetime(forecast_date) \\\n",
    "              + dt.timedelta(days=int(e)) for e in fc.time.values]\n",
    "\n",
    "anim = xvid(fc, 15, figsize=4, mask=land_mask)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "*Provide 3-5 bullet points summarising the main aspects of the dataset and tools covered in the notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentence 1 e.g. `tool-name` to perform...\n",
    "* Sentence 2 e.g. `tool-name` to perform..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "**Dataset**: Type here details of dataset(s) version.\n",
    "\n",
    "**Codebase**: Type here details of codebase version (only for notebooks categorised under modelling/preprocesing/post-processing themes).\n",
    "\n",
    "**License**: The code in this notebook is licensed under the MIT License. The Environmental Data Science book is licensed under the Creative Commons by Attribution 4.0 license. See further details [here](https://github.com/alan-turing-institute/environmental-ds-book/blob/master/LICENSE.md).\n",
    "\n",
    "**Contact**: If you have any suggestion or report an issue with this notebook, feel free to [create an issue](https://github.com/alan-turing-institute/environmental-ds-book/issues/new/choose) or send a direct message to [environmental.ds.book@gmail.com](mailto:environmental.ds.book@gmail.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icenet\n",
    "icenet_version = icenet.__version__\n",
    "print(f'IceNet version: {icenet_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "print(f'Last tested: {date.today()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
